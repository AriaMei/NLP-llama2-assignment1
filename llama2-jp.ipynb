{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2df92583f44640f38112db08ea30cc5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ef56642df394267850e820ae578d3d7","IPY_MODEL_fb11f7b6a113420cbf9353c79f434ad5","IPY_MODEL_b039f8470ffa4535ad065fc0eda886fd"],"layout":"IPY_MODEL_d2f6deec117a4358ad064db89f727bc1"}},"9ef56642df394267850e820ae578d3d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_459a84f78164433884ba5f470a9e1417","placeholder":"​","style":"IPY_MODEL_19620102f85d4c6a8b48e1078a180e95","value":"100%"}},"fb11f7b6a113420cbf9353c79f434ad5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8995660b5da643a495cc5d8d23104913","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ae1f05679b84ac9bac39c77485e6b01","value":63}},"b039f8470ffa4535ad065fc0eda886fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e063f6ac5f4a44b88271e700e985185a","placeholder":"​","style":"IPY_MODEL_325cd0c8a932466d9c115ffba2904bba","value":" 63/63 [00:48&lt;00:00,  1.54it/s]"}},"d2f6deec117a4358ad064db89f727bc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"459a84f78164433884ba5f470a9e1417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19620102f85d4c6a8b48e1078a180e95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8995660b5da643a495cc5d8d23104913":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ae1f05679b84ac9bac39c77485e6b01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e063f6ac5f4a44b88271e700e985185a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"325cd0c8a932466d9c115ffba2904bba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a50911f63024bf59da9873dd6d1a912":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9b6ee204f4a457ab9863c0f3baf4864","IPY_MODEL_40a0c17e7fde4bf2862ecda678d5e963","IPY_MODEL_e0cdab58ab0c4936878db9d524c8726c"],"layout":"IPY_MODEL_b2143d465f7047629cdfe9777e3a286b"}},"e9b6ee204f4a457ab9863c0f3baf4864":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e56cd9f3c4124896a1f04d55b19888ea","placeholder":"​","style":"IPY_MODEL_67c58390dc914bb6a8db182ab1943157","value":"100%"}},"40a0c17e7fde4bf2862ecda678d5e963":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76f64cb2e08e453d977b20150745fe41","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35fe5e0e1ce14594b643f9679ea2b7f3","value":63}},"e0cdab58ab0c4936878db9d524c8726c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5769bc0ec3e4662b08fc27d8dda40c8","placeholder":"​","style":"IPY_MODEL_91f68453eca74af8804b1871a20a0b26","value":" 63/63 [01:18&lt;00:00,  1.07s/it]"}},"b2143d465f7047629cdfe9777e3a286b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e56cd9f3c4124896a1f04d55b19888ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67c58390dc914bb6a8db182ab1943157":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76f64cb2e08e453d977b20150745fe41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35fe5e0e1ce14594b643f9679ea2b7f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5769bc0ec3e4662b08fc27d8dda40c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91f68453eca74af8804b1871a20a0b26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35d813e7b32f44e49d38155e4f950c86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4245c28c8eec4edd921b1755ad1ad45b","IPY_MODEL_2c286ad6343a475d8516ac4779b208ac","IPY_MODEL_62beee732f344d1fabf7e213930ad775"],"layout":"IPY_MODEL_037842bf542143038269aedb14c51c0d"}},"4245c28c8eec4edd921b1755ad1ad45b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f004def454c46f4a489d83a265577f6","placeholder":"​","style":"IPY_MODEL_5407caa8f2df48179033c1d0d6daebfe","value":"100%"}},"2c286ad6343a475d8516ac4779b208ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa04910012354222b3f1a49ed5e33203","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b45c1921cebe42418c9bac889be111de","value":1}},"62beee732f344d1fabf7e213930ad775":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_645cdc1efc3344dc8625f9ed98ee4433","placeholder":"​","style":"IPY_MODEL_b2f56d482fdd43dea48c9f649680e11b","value":" 1/1 [00:00&lt;00:00, 19.33it/s]"}},"037842bf542143038269aedb14c51c0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f004def454c46f4a489d83a265577f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5407caa8f2df48179033c1d0d6daebfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa04910012354222b3f1a49ed5e33203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b45c1921cebe42418c9bac889be111de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"645cdc1efc3344dc8625f9ed98ee4433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2f56d482fdd43dea48c9f649680e11b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9353483,"sourceType":"datasetVersion","datasetId":5670072},{"sourceId":9375116,"sourceType":"datasetVersion","datasetId":5686567},{"sourceId":9377523,"sourceType":"datasetVersion","datasetId":5688431},{"sourceId":196364513,"sourceType":"kernelVersion"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment 1: Build a Toy Llama-2 Language Model\n\n> CISC7021 Applied Natural Language Processing (2024/2025)\n\nIn this assignment, we will prepare a toy language model that employs the **Llama-2** architecture and evaluate the perplexity of the data set.\n\nWe will learn how to perform continual pre-training of a base language model using the PyTorch and Hugging Face libraries. Detailed instructions for building this language model can be found in the attached notebook file.\n\nAcknowledgement: The base model checkpoint is converted from [llama2.c](https://github.com/karpathy/llama2.c) project. The data instances were sampled from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.\n\n---\n\n🚨 Please note that running this on CPU may be slow. If running on Google Colab or Kaggle, you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n\n---\n\nWe start by doing a `pip install` of all required libraries.\n- 🤗 `transformers`, `datasets`, `accelerate` are Huggingface libraries.\n- By default, Colab has `transformers`, `pytorch` libraries installed. If you are using a local machine, please install them via `pip` or `conda`.","metadata":{"id":"qycf3ikvkUqP"}},{"cell_type":"code","source":"#!pip install torch torchvision torchaudio\n#!pip install transformers","metadata":{"id":"UOhVvTEaa_b0","execution":{"iopub.status.busy":"2024-09-12T10:33:49.101454Z","iopub.execute_input":"2024-09-12T10:33:49.101846Z","iopub.status.idle":"2024-09-12T10:33:49.106436Z","shell.execute_reply.started":"2024-09-12T10:33:49.101811Z","shell.execute_reply":"2024-09-12T10:33:49.105462Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets accelerate -q","metadata":{"id":"mNC4vO-JkUqQ","execution":{"iopub.status.busy":"2024-09-12T10:33:49.108153Z","iopub.execute_input":"2024-09-12T10:33:49.108509Z","iopub.status.idle":"2024-09-12T10:34:03.707015Z","shell.execute_reply.started":"2024-09-12T10:33:49.108468Z","shell.execute_reply":"2024-09-12T10:34:03.705983Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### (Optional) Uploading the model/data to Google Colab or Kaggle.\n\nPlease upload your dataset and model to computational platforms if you are using Colab or Kaggle environments.\n\nFor Colab users, you can mount your Google Drive files by running the following code snippet:","metadata":{"id":"2WzqztoXkUqQ"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"kRSpL18W_Zfa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c489035e-c816-4552-cea9-133bc0492bf5","execution":{"iopub.status.busy":"2024-09-12T10:34:03.708714Z","iopub.execute_input":"2024-09-12T10:34:03.709161Z","iopub.status.idle":"2024-09-12T10:34:03.714503Z","shell.execute_reply.started":"2024-09-12T10:34:03.709115Z","shell.execute_reply":"2024-09-12T10:34:03.713151Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Necessary Packages, Environment Setups","metadata":{"id":"dqvZRLZYkUqR"}},{"cell_type":"code","source":"import torch\nimport transformers\n\nfrom typing import List, Optional, Tuple, Union\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\nfrom transformers import Trainer, TrainingArguments\nfrom itertools import chain\nfrom datasets import load_dataset\n\nfrom tqdm.notebook import tqdm\nfrom torch.nn import CrossEntropyLoss","metadata":{"id":"Sa1iUH1ykUqR","execution":{"iopub.status.busy":"2024-09-13T15:22:52.908505Z","iopub.execute_input":"2024-09-13T15:22:52.909449Z","iopub.status.idle":"2024-09-13T15:23:12.786841Z","shell.execute_reply.started":"2024-09-13T15:22:52.909376Z","shell.execute_reply":"2024-09-13T15:23:12.785702Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Please set the correct file path based on your environment.\n\n- If you are using Colab, the path may be: `/content/drive/MyDrive/xxxxxx`\n- If you are using Kaggle, the path may be: `/kaggle/input/xxxxxx`","metadata":{"id":"TBzFKWGZkUqR"}},{"cell_type":"code","source":"# Please set the correct file path based on your environment.\nTRAIN_FILE = '/kaggle/input/englishtochinesetext/Model and Datasets/data/zh_train.jsonl'\nVALIDATION_FILE = '/kaggle/input/englishtochinesetext/Model and Datasets/data/zh_dev.jsonl'\nTEST_FILE = '/kaggle/input/englishtochinesetext/Model and Datasets/data/zh_test.jsonl'\nPT_TEST_FILE = '/kaggle/input/englishtochinesetext/Model and Datasets/data/pt_test.jsonl'\nEN_TEST_FILE = '/kaggle/input/englishtochinesetext/Model and Datasets/data/en_test.jsonl'\nMODEL_FOLDER = \"/kaggle/input/englishtochinesetext/Model and Datasets/llama-42m\"\nJP_TRAIN_FILE = '/kaggle/input/jpdata-o/jp_train.jsonl'\nJP_TEST_FILE = '/kaggle/input/datsetjp-en-cn/Model and Datasets/data/jp_test.jsonl'\nJP_VALIDATION_FILE='/kaggle/input/datsetjp-en-cn/Model and Datasets/data/jp_dev.jsonl'","metadata":{"id":"mYj0DvSgGmWq","execution":{"iopub.status.busy":"2024-09-13T15:23:12.789132Z","iopub.execute_input":"2024-09-13T15:23:12.790123Z","iopub.status.idle":"2024-09-13T15:23:12.796703Z","shell.execute_reply.started":"2024-09-13T15:23:12.790063Z","shell.execute_reply":"2024-09-13T15:23:12.795740Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Load the model checkpoint into either a GPU or CPU (training will be slow on CPU, but decoding will be fair).","metadata":{"id":"ElstHAMjkUqS"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device type: {device}\")\n\nmodel_path = MODEL_FOLDER\n# Load model from local files\nmodel = LlamaForCausalLM.from_pretrained(model_path).to(device)\n# Load tokenizer from local files\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZF-0tQYDjvPl","outputId":"70706b9e-30c1-470e-f8fc-5ba51c494bc7","execution":{"iopub.status.busy":"2024-09-13T15:23:25.273477Z","iopub.execute_input":"2024-09-13T15:23:25.274259Z","iopub.status.idle":"2024-09-13T15:23:27.584436Z","shell.execute_reply.started":"2024-09-13T15:23:25.274222Z","shell.execute_reply":"2024-09-13T15:23:27.583410Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see from the statistics, this model is much smaller than Llama-2 but shares the same decoder-only architecture.\n\n\n😄 **You do not need to check complex details!** We just present the architecture and number of parameters here.","metadata":{"id":"0g81Fac6kUqS"}},{"cell_type":"code","source":"total_para = sum(v.numel() for k, v in model.state_dict().items() if k != 'model.embed_tokens.weight') / 1e6\nprint(model)\nprint(f\"#Parameters: {total_para:.2f}M\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dqxUbbA2u2uc","outputId":"dd426913-1155-44dd-c932-b20683038c00","execution":{"iopub.status.busy":"2024-09-13T15:23:28.101909Z","iopub.execute_input":"2024-09-13T15:23:28.102256Z","iopub.status.idle":"2024-09-13T15:23:28.112017Z","shell.execute_reply.started":"2024-09-13T15:23:28.102222Z","shell.execute_reply":"2024-09-13T15:23:28.110875Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 512)\n    (layers): ModuleList(\n      (0-7): 8 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((512,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n)\n#Parameters: 41.69M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 1: Decoding","metadata":{"id":"Z6LPYv5KkUqS"}},{"cell_type":"markdown","source":"\nIf you are familar with the usage of `model.generate()` function in transformer library, please feel free to jump to [Task 1 Playground](#scrollTo=Task_1_Playground).\n\n\n#### 💡Tutorials: model.generate() function.\n---\nMinimal example:\n\n```python\nprompt = \"Once upon a time, \" # Input, prefix of generation\n```\n\n**Step 1**: Encode raw text using tokenizer model.\n```python\ntokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n```\n\n**Step 2**: Set decoding hyper-parameters. Get the model output.\n```python\noutput_ids = model.generate(tokenized_input, do_sample=True, max_new_tokens=300, temperature=0.6)\n```\nImportant parameters:\n- `max_new_tokens`: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n- `temperature`: The value of temperature used to modulate the next token probabilities. Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n- `do_sample`: `do_sample=False` is using greedy decoing strategy. To enable greedy decoding, we also need to set other sampling parameters `top_p`, `temperature` as `None`.\n- [If you are interested in other decoding algorithms, please refer to this link for setting parameters.](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationConfig)\n\n**Step 3**: Convert model outputs into raw text.\n```python\noutput_text = tokenizer.decode(output_ids[0])\n```\nor (when input instances >=1)\n```python\noutput_text = tokenizer.batch_decode(output_ids)\n```\nImportant parameters:\n- Setting `skip_special_tokens=True` will prevent special tokens, such as `<s>`, from appearing in the results..\n\n---\n","metadata":{"id":"PxuzZuOrR4mE"}},{"cell_type":"markdown","source":"To understand the outputs of each step, let us do a simple generation task step by step! (Note: the base model is only able to produce fluent story text).","metadata":{"id":"tkvkY3LPkUqT"}},{"cell_type":"code","source":"prompt = \"Once upon a time, Stella Lou had a dream.\" # Feel free to use other generation prefix","metadata":{"id":"Gf6Q9qcgkUqT","execution":{"iopub.status.busy":"2024-09-13T15:23:31.901950Z","iopub.execute_input":"2024-09-13T15:23:31.902466Z","iopub.status.idle":"2024-09-13T15:23:31.907824Z","shell.execute_reply.started":"2024-09-13T15:23:31.902412Z","shell.execute_reply":"2024-09-13T15:23:31.906782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Step 1: Encode raw text using tokenizer model. Run tokenization and covert strings into token ids in vocabulary.\ntokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n# See the tokenized results.\nprint(tokenized_input)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEKNYuWiJKIB","outputId":"3c01550c-3b35-4c2d-c7b7-1fb44af22f5d","execution":{"iopub.status.busy":"2024-09-13T15:23:33.132226Z","iopub.execute_input":"2024-09-13T15:23:33.133213Z","iopub.status.idle":"2024-09-13T15:23:33.153475Z","shell.execute_reply.started":"2024-09-13T15:23:33.133152Z","shell.execute_reply":"2024-09-13T15:23:33.152298Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n           263, 12561, 29889]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 2: Set decoding hyperparameters.\n\n# For greedy decoding\nmax_new_tokens = 300\ndo_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\ntemperature = None\n\n# call generation function model.generate()\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n    top_p=None,\n)\n\n# The decoded results are token ids.\nprint(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\nprint(output_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlmCc71dElxz","outputId":"5cf2d468-1269-492a-f72f-0a92742a139b","execution":{"iopub.status.busy":"2024-09-13T15:23:34.818504Z","iopub.execute_input":"2024-09-13T15:23:34.819235Z","iopub.status.idle":"2024-09-13T15:23:37.810934Z","shell.execute_reply.started":"2024-09-13T15:23:34.819193Z","shell.execute_reply":"2024-09-13T15:23:37.810002Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"====================Token IDs====================\ntensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n           263, 12561, 29889,  2296,  5131,   304,   367,   263, 12456,   985,\n         29889,  2296,  5131,   304, 19531,   263,  9560, 10714,   322,   263,\n           528,  4901, 20844, 29889,  1205,  1183,   471,  2086,  2319,   322,\n           278, 10714,   471,  2086,  4802, 29889,    13,  6716,  2462, 29892,\n           624,  3547,  4446,   263,  4802, 29892,   528,  4901, 10714,   297,\n           263,  3787, 29889,  2296,  4433,   902, 16823,   565,  1183,  1033,\n           505,   372, 29889,  2439, 16823,  1497,  4874,   322, 18093,   372,\n           363,   902, 29889,    13,   855,  3547,   471,   577,  9796, 29889,\n          2296,  1925,   373,   278, 10714,   322,  3252,   381,   839,  2820,\n         29889,  2296,  7091,   763,   263,  1855, 12456,   985, 29889,    13,\n          6246,   769, 29892,  1554,  8515,  9559, 29889,   624,  3547,  4687,\n           304,  4459,   270,   466,  1537, 29889,  2296,  8496, 29915, 29873,\n          2317,   701,  7812, 29889,  2296,  7091,   763,  1183,   471, 10917,\n          1076,  2820,   322,  2820, 29889,    13,   855,  3547, 29915, 29879,\n         16823,  4446,   902,   322,  1497, 29892,   376,   855,  3547, 29892,\n           366,   817,   304,  2125,   263,  2867, 29889,   887,  1106,   270,\n           466,  1537,  1213,    13,   855,  3547,  3614,  1283,   278, 10714,\n           322,  6568,  1623,   373,   278, 11904, 29889,  2296,  5764,   902,\n          5076,   322,  3614,   263,  6483, 16172, 29889,  2860,   263,  2846,\n          6233, 29892,  1183,  7091,  2253, 29889,    13,   855,  3547, 25156,\n           322,  1497, 29892,   376, 29924,   290, 29892,   306, 29915, 29885,\n          7960,   304,   367,   263, 12456,   985,  1449,  3850,     1]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 3: Convert model outputs into raw text.\n# decode token ids into tokens\nprint(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\n# We only have one input instance. So we directly decode the first item of model output, i.e., `output_ids[0]`.\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQqRmKrXIYM1","outputId":"124d1235-b532-4f7a-f51a-75f4ee36cb11","execution":{"iopub.status.busy":"2024-09-13T15:25:55.188925Z","iopub.execute_input":"2024-09-13T15:25:55.189350Z","iopub.status.idle":"2024-09-13T15:25:55.196966Z","shell.execute_reply.started":"2024-09-13T15:25:55.189308Z","shell.execute_reply":"2024-09-13T15:25:55.195892Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"====================Decoded Results====================\nOnce upon a time, Stella Lou had a dream. She wanted to be a princess. She wanted to wear a beautiful dress and a shiny crown. But she was too small and the dress was too big.\nOne day, Stella saw a big, shiny dress in a store. She asked her mom if she could have it. Her mom said yes and bought it for her.\nStella was so happy. She put on the dress and twirled around. She felt like a real princess.\nBut then, something strange happened. Stella started to feel dizzy. She couldn't stand up straight. She felt like she was spinning around and around.\nStella's mom saw her and said, \"Stella, you need to take a break. You look dizzy.\"\nStella took off the dress and lay down on the floor. She closed her eyes and took a deep breath. After a few minutes, she felt better.\nStella smiled and said, \"Mom, I'm ready to be a princess again!\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Another pipeline example: Sampling decoding with temperature.","metadata":{"id":"pRNuqdJkRvxw"}},{"cell_type":"code","source":"prompt = \"Stella Lou hurt herself.\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\n# The value of temperature used to modulate the next token probabilities.\n# Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\ntemperature = 0.6\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCqO5bkl341O","outputId":"bad5340c-a6e7-4ce0-d4eb-389f0ea80c29","execution":{"iopub.status.busy":"2024-09-13T15:25:59.482675Z","iopub.execute_input":"2024-09-13T15:25:59.483445Z","iopub.status.idle":"2024-09-13T15:26:01.365223Z","shell.execute_reply.started":"2024-09-13T15:25:59.483383Z","shell.execute_reply":"2024-09-13T15:26:01.364282Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<s> Stella Lou hurt herself. She had been playing in the park with her friends, but she had not been careful. She had run too fast and fallen down.\n\"Ouch!\" she cried.\nHer mom came running over. \"What happened?\" she asked.\n\"I fell down,\" Stella said, tears streaming down her face.\nHer mom hugged her and said, \"It's ok. Let's get you home and make you feel better.\"\nStella smiled and nodded. Her mom took her home and put a bandage on her knee. She gave her a big hug and said, \"You'll be okay.\"\nStella felt a little better. She was glad her mom was there to help her. She knew she would be more careful next time she played in the park.<s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Task 1 Playground\n\n---\n\n📚 Task 1: Please generate English stories using various prompts and decoding settings. Please feel free to explore any interesting phenomena, such as the impact of different prompts and the effects of various decoding algorithms and parameters. For example, quantify the text properties using linguistic-driven metrics like story length and Type-Token Ratio (TTR). In addition to objective metrics, you are encouraged to discuss your findings based on subjective case studies.\n\nWe provide two types of skeleton code: one that takes a single prompt as input and another that can process batched inputs and decoding. Please use the version that best fits your preferences and data types.\n\n---","metadata":{"id":"LT7or09aSBhp"}},{"cell_type":"code","source":"# Skeleton Code: Single input (same as previous code blocks)\n\nprompt = \"\" # ⬅️ try to construct different prompts.\n\n# ⬇️ Try to tune different decoding hyperparameters.\n# You can also add more hyperparameters like `top_p`, `top_k`.\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.6\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    top_k=10,  # 从概率最高的 50 个候选词中采样\n    top_p=0.9,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"id":"DNvZ5Q6rYeC4","execution":{"iopub.status.busy":"2024-09-13T15:35:28.654347Z","iopub.execute_input":"2024-09-13T15:35:28.655074Z","iopub.status.idle":"2024-09-13T15:35:31.358430Z","shell.execute_reply.started":"2024-09-13T15:35:28.655034Z","shell.execute_reply":"2024-09-13T15:35:31.357473Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<s> Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she went to the park with her mom. They played on the swings and went down the slide. \nAfter a while, Lily's mom said it was time to go home. But Lily didn't want to leave yet. She said, \"Mommy, can we stay a little longer? I want to play more.\" \nHer mom said, \"I'm sorry, Lily. We have to go home now. It's getting late and we need to eat dinner.\" \nLily felt sad and didn't want to leave. She said, \"But mommy, I want to stay and play more. Please?\" \nHer mom said, \"I know it's hard, but we have to go. We can come back another day.\" \nLily understood and they walked home together. As they walked, Lily looked up at the sky and said, \"Mommy, the sky is so blue. It's like a big, open sky.\" \nHer mom smiled and said, \"Yes, it is. And when we get home, we can have a yummy dinner and watch a movie together.\" \nLily felt happy and excited to spend more time with her mom.<s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Skeleton Code: Bacthed input-output\n\nprompts = [\"Once upon a time,\", \"Tom is a cute kitty.\"]  # ⬅️ try to construct different prompts.\n\nbatch_size = 2 # If you have multiple data inputs, please control the batch size to prevent out-of-memory issues.\n\n# ⬇️ Try to tune different decoding hyperparameters.\n# You can also add more hyperparameters like `top_p`, `top_k`.\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.6\n\nfor i in range(0, len(prompts), batch_size):\n    batch_input = prompts[i:i+batch_size]\n    tokenized_input = tokenizer(batch_input, return_tensors=\"pt\", padding=True).to(device)\n\n    # For decoder-only models, batched inputs of model.generate() should be in the format of input_ids.\n    output_ids = model.generate(\n        tokenized_input[\"input_ids\"],\n        max_new_tokens=max_new_tokens,\n        eos_token_id=1,\n        do_sample=do_sample,\n        temperature=temperature,\n    )\n    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n\n    for idx, result in enumerate(output_text):\n        print(f\"{result}\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80jDK-9uP1IO","outputId":"ceed92d8-c965-4792-b121-ba5bbabab913","execution":{"iopub.status.busy":"2024-09-13T15:35:37.601012Z","iopub.execute_input":"2024-09-13T15:35:37.602086Z","iopub.status.idle":"2024-09-13T15:35:40.537937Z","shell.execute_reply.started":"2024-09-13T15:35:37.602044Z","shell.execute_reply":"2024-09-13T15:35:40.536815Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Once upon a time,ie was playing in the park. She saw a big, black bird flying above her. It was a very big black bird and it was very loud. The bird was so loud that it made the ground shake.\nSuddenly, the bird flew down and landed right in front of the little girl. The girl was scared and she started to cry. The bird said, \"Don't be scared, I'm here to help you.\" The bird then flew up and grabbed the girl's shoe with its beak. The shoe fell down and the girl was happy.\nThe bird then said, \"I have an idea. Let's go and find a big, black bird that can help us.\" The girl was excited and she said, \"Yes!\"\nThe bird flew off and soon came back with a big, black bird. The girl was scared at first, but then the big, black bird said, \"Don't be scared. I'm here to help you.\" The bird then flew around the park, making a lot of noise. The little girl laughed and the bird flew away.\nThe little girl was happy and she thanked the big, black bird for helping her. She waved goodbye and went home. From that day on, the little girl was never scared of the big, black bird again.\n\nTom is a cute kitty. He likes to play with his toys and sleep on his bed. He has a friend named Lily, who is a dog. Lily likes to run and bark and chase balls.\nOne day, Tom and Lily are in the garden. They see a big bird on a tree. The bird is black and white and has a long tail. Tom wants to catch the bird. He runs to the tree and jumps on it. He tries to climb the tree, but the branch is too high.\n\"Tom, be careful!\" Lily says. \"You can fall and hurt yourself!\"\nBut Tom does not listen. He wants to catch the bird. He jumps and swings his paw, but he cannot reach the branch. He falls down and lands on the grass. He is not hurt, but he is sad.\nLily runs to Tom and licks his face. She says, \"It's okay, Tom. You are still cute. The bird is not your friend, it is a wild animal. It does not want to play with you.\"\nTom looks at Lily and meows. He is sorry for being mean. He says, \"I'm sorry, Lily. You are right. I was silly. I should not chase the bird. I should be happy with what I have.\"\nLily smiles and says, \"It's okay, Tom. I\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### What about other languages?","metadata":{"id":"rhfIINEMSIjp"}},{"cell_type":"markdown","source":"Oops! This English language model cannot generate stories in other languages!\n\nWhy? Let us evaluate the perplexity of different languages in the next task.","metadata":{"id":"pYlvKScxkUqT"}},{"cell_type":"code","source":"prompt = \"从前有一只小兔子乖乖\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0jb_ZqW_ixo","outputId":"e5f718d0-8025-438e-e5d5-79eb2ce7932c","execution":{"iopub.status.busy":"2024-09-12T17:48:53.806181Z","iopub.execute_input":"2024-09-12T17:48:53.807116Z","iopub.status.idle":"2024-09-12T17:48:55.877093Z","shell.execute_reply.started":"2024-09-12T17:48:53.807074Z","shell.execute_reply":"2024-09-12T17:48:55.875993Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<s> 从前有一只小兔子乖乖 couldn't wait to get to the park. He put on his shoes and ran outside.\nWhen he got to the park, he saw a big slide. He wanted to go down it, but he was scared. He looked around and saw a big tree. He thought it would be fun to climb it.\nHe started to climb the tree, but it was very high. He got scared and started to cry. He wanted to go down the slide, but he was too scared.\nSuddenly, a big bird flew down and landed on the tree. It looked at him and said, \"Don't be scared. I'll help you.\" The bird flew down and helped him go down the slide.\nWhen he got to the bottom, he was so happy. He thanked the bird and ran off to play. He had a lot of fun at the park.<s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 2: Perplexity Evaluation","metadata":{"id":"DYst_QWTkUqU"}},{"cell_type":"markdown","source":"#### Background\n\n---\n\nThe perplexity serves as a key metric for evaluating language models. It quantifies how well a model predicts a sample, with lower perplexity indicating better performance. For a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, the perplexity is defined mathematically as:\n\n$$\\text{Perplexity}(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i=1}^t \\log p_\\theta (x_i | x_{<i}) \\right)$$\n\nHere, $p_\\theta(x_i | x_{<i})$ represents the probability of a token $ x_i $ given its preceding tokens, and the formulation incorporates the average log probability across the sequence.\n\n---\n\n⚠️ Please make sure to **run the following cell first** to define the evaluation function.\n\n😄 **You do not need to check these complex details! Too hard for beginners!** However, if you are interested, you can compare the following code with the explanations above to better understand how to implement PPL evaluation using PyTorch.","metadata":{"id":"T6NxwPeaBzOB"}},{"cell_type":"code","source":"# The following code was adapted from the `evaluate` library. Licensed under the Apache License, Version 2.0 (the \"License\").\n# We modify them to avoid causing serious memory issues in the Colab environment.\n\ndef compute_ppl(\n        model, tokenizer, inputs, device, batch_size: int = 16, add_start_token: bool = True, max_length=None\n):\n\n    if device is not None:\n        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n        if device == \"gpu\":\n            device = \"cuda\"\n    else:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # if batch_size > 1 (which generally leads to padding being required), and\n    # if there is not an already assigned pad_token, assign an existing\n    # special token to also be the padding token\n    if tokenizer.pad_token is None and batch_size > 1:\n        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n        # check that the model already has at least one special token defined\n        assert (\n            len(existing_special_tokens) > 0\n        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n        # assign one of the special tokens to also be the pad token\n        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n\n    if add_start_token and max_length:\n        # leave room for <BOS> token to be added:\n        assert (\n            tokenizer.bos_token is not None\n        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n        max_tokenized_len = max_length - 1\n    else:\n        max_tokenized_len = max_length\n\n    encodings = tokenizer(\n        inputs,\n        add_special_tokens=False,\n        padding=True,\n        truncation=True if max_tokenized_len else False,\n        max_length=max_tokenized_len,\n        return_tensors=\"pt\",\n        return_attention_mask=True,\n    )\n\n    encoded_texts = encodings[\"input_ids\"]\n    attn_masks = encodings[\"attention_mask\"]\n\n    # check that each input is long enough:\n    if add_start_token:\n        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n    else:\n        assert torch.all(\n            torch.ge(attn_masks.sum(1), 2)\n        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n\n    ppls = []\n    loss_fct = CrossEntropyLoss(reduction=\"none\")\n\n    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n        end_index = min(start_index + batch_size, len(encoded_texts))\n        encoded_batch = encoded_texts[start_index:end_index].to(device)\n        attn_mask = attn_masks[start_index:end_index].to(device)\n\n        if add_start_token:\n            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n            attn_mask = torch.cat(\n                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n            )\n\n        labels = encoded_batch\n\n        with torch.no_grad():\n            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n\n            shift_logits = out_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n\n            perplexity_batch = torch.exp(\n                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n                / shift_attention_mask_batch.sum(1)\n            )\n\n            ppls += perplexity_batch.tolist()\n\n    del encoded_batch, attn_mask\n    if device == \"cuda\":\n        torch.cuda.empty_cache()\n\n    return {\"perplexities\": ppls, \"mean_perplexity\": sum(ppls)/float(len(ppls))}","metadata":{"id":"bEIdmBRsJaP9","execution":{"iopub.status.busy":"2024-09-13T15:35:57.462452Z","iopub.execute_input":"2024-09-13T15:35:57.462825Z","iopub.status.idle":"2024-09-13T15:35:57.479653Z","shell.execute_reply.started":"2024-09-13T15:35:57.462789Z","shell.execute_reply":"2024-09-13T15:35:57.478678Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"\n#### 💡Tutorials: compute_ppl() function.\n\n---\nMinimal example:\n\n```python\ntest_dataset = [\"Once upon a time,\"]\n\ncompute_ppl(\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    inputs=test_dataset,\n    batch_size = 16\n)\n```\n\nImportant parameters:\n- `inputs`: list of input text, each separate text snippet is one list entry.\n- `batch_size`: the batch size to run evaluations.\n\nReturns:\n- `perplexity`: `{\"perplexities\": [x.x, x.x, ...], \"mean_perplexity\": x.x}` dictionary containing the perplexity scores for the texts in the input list, as well as the mean perplexity. .\n\n\n---","metadata":{"id":"9TscJWpxB2XH"}},{"cell_type":"markdown","source":"#### Task 2 Playground\n\n---\n\n📚 Task 2: Evaluate the perplexity. Ensure that you evaluate both the English and Chinese test data we provided. You are encouraged to collect more diverse text data and discuss your findings regarding the language understanding capacity of the base model.\n\n\nNote: If you want to reuse the evaluation codes for JSONL data, please structure the content as follows:\n```json\n{\"text\": \"one data\"}\n{\"text\": \"two data.\"}\n...\n```\n**You may find that the PPL value for Chinese text is significantly higher than that for English text. This is evidence that the base model cannot generate a Chinese story at the end of the last task.**\n\n---","metadata":{"id":"2yDr5pIVCHCP"}},{"cell_type":"code","source":"# Skeleton Code: Evaluate the perplexity (PPL) on a list of raw text.\n\ntest_dataset = [\"Once upon a time,\", \"Tom is a cute kitty.\"] # ⬅️ you can use your examples / or read from raw text file\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["35d813e7b32f44e49d38155e4f950c86","4245c28c8eec4edd921b1755ad1ad45b","2c286ad6343a475d8516ac4779b208ac","62beee732f344d1fabf7e213930ad775","037842bf542143038269aedb14c51c0d","7f004def454c46f4a489d83a265577f6","5407caa8f2df48179033c1d0d6daebfe","fa04910012354222b3f1a49ed5e33203","b45c1921cebe42418c9bac889be111de","645cdc1efc3344dc8625f9ed98ee4433","b2f56d482fdd43dea48c9f649680e11b"]},"id":"QSWMQtwDSdIm","outputId":"0841a9fc-fe40-499b-aae7-15d202149077","execution":{"iopub.status.busy":"2024-09-13T15:36:02.968518Z","iopub.execute_input":"2024-09-13T15:36:02.969289Z","iopub.status.idle":"2024-09-13T15:36:03.051597Z","shell.execute_reply.started":"2024-09-13T15:36:02.969252Z","shell.execute_reply":"2024-09-13T15:36:03.050658Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235b4ec997384dc2adc03c3da138d4da"}},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 10.68\n","output_type":"stream"}]},{"cell_type":"code","source":"#Japenese test set.\ndata_file = JP_TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Japanese Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n\ndata_file = EN_TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(English Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n# Chinese test set.\ndata_file = TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Chinese Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n# pt test set.\ndata_file = PT_TEST_FILE # ⬅️ you can change your file path\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\ndataset_ppl = results['mean_perplexity']\nprint(f\"(Portuguese Text) Test Perplexity: {dataset_ppl:.2f}\")\n\n\n# Try your own data file!","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["2df92583f44640f38112db08ea30cc5e","9ef56642df394267850e820ae578d3d7","fb11f7b6a113420cbf9353c79f434ad5","b039f8470ffa4535ad065fc0eda886fd","d2f6deec117a4358ad064db89f727bc1","459a84f78164433884ba5f470a9e1417","19620102f85d4c6a8b48e1078a180e95","8995660b5da643a495cc5d8d23104913","2ae1f05679b84ac9bac39c77485e6b01","e063f6ac5f4a44b88271e700e985185a","325cd0c8a932466d9c115ffba2904bba","1a50911f63024bf59da9873dd6d1a912","e9b6ee204f4a457ab9863c0f3baf4864","40a0c17e7fde4bf2862ecda678d5e963","e0cdab58ab0c4936878db9d524c8726c","b2143d465f7047629cdfe9777e3a286b","e56cd9f3c4124896a1f04d55b19888ea","67c58390dc914bb6a8db182ab1943157","76f64cb2e08e453d977b20150745fe41","35fe5e0e1ce14594b643f9679ea2b7f3","b5769bc0ec3e4662b08fc27d8dda40c8","91f68453eca74af8804b1871a20a0b26"]},"id":"4a_qx_9LLHYI","outputId":"a66756f4-6ae8-4d8b-e1dd-7697d6e75966","execution":{"iopub.status.busy":"2024-09-13T15:36:07.818173Z","iopub.execute_input":"2024-09-13T15:36:07.819138Z","iopub.status.idle":"2024-09-13T15:39:16.729232Z","shell.execute_reply.started":"2024-09-13T15:36:07.819081Z","shell.execute_reply":"2024-09-13T15:39:16.728307Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87106bb207314d2895962b7f61469bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdd5f4d8658441e9b6bad1168382455"}},"metadata":{}},{"name":"stdout","text":"(Japanese Text) Test Perplexity: 78686.17\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3087a2e075404eaa8ff8df5fe05a6e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd9e0eb62eca4d29a3f28e80494cc010"}},"metadata":{}},{"name":"stdout","text":"(English Text) Test Perplexity: 4.14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e2faff7b6343ecb2f575d48e9838ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974cb56328dd42e09e5d1c81080f660a"}},"metadata":{}},{"name":"stdout","text":"(Chinese Text) Test Perplexity: 70030.42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad9cfb93d6a546d79e537d46da012115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01da3f9770d461a9afc1d78aa589096"}},"metadata":{}},{"name":"stdout","text":"(Portuguese Text) Test Perplexity: 25050.81\n","output_type":"stream"}]},{"cell_type":"code","source":"# 🚨 Release gpu cache before training the model\nif device == \"cuda\":\n    for i in range(torch.cuda.device_count()):\n        torch.cuda.set_device(i)\n        torch.cuda.empty_cache()","metadata":{"id":"MVELmXzJP7GS","execution":{"iopub.status.busy":"2024-09-13T04:53:18.348412Z","iopub.execute_input":"2024-09-13T04:53:18.348820Z","iopub.status.idle":"2024-09-13T04:53:18.465077Z","shell.execute_reply.started":"2024-09-13T04:53:18.348782Z","shell.execute_reply":"2024-09-13T04:53:18.463956Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Task 3: Continual Pre-training (in Chinese or in another language you are proficient in)\n\nCurrently, our base English LM is proficient in English but lacks the capability to generate or comprehend other languages (e.g., Chinese). The objective of this task is to enhance a base English LM by continually pre-training it with text in another language. This process aims to enable the model to understand and generate mini-story in another language.\n\nWe have provided 10,000 Chinese training samples. The training process for any language is the same. We have included useful resource links (in Assignment description PDF) to help you create additional data. If you encounter any issues in creating a dataset in another language, please do not hesitate to contact us.\n\nWe have implemented data preprocessing and the training pipeline, so you are not required to optimize these components. Instead, focus on tuning the training hyperparameters and observe the changes in model performance.\n\n\n---\n\n⚠️ Please **make sure to run the following cell first to pre-process data**.\n\n😄 You do not need to check the details of whole pipeline construction! Please pay attention to the hyper-parameters of `trainer`.","metadata":{"id":"_gilWue9kUqU"}},{"cell_type":"markdown","source":"#### Preprocess Data\nHere, we preprocess (tokenize and group) the text for the subsequent evaluation and pre-training phases.","metadata":{"id":"dzYCSeGVkUqT"}},{"cell_type":"markdown","source":"Load prepared Chinese dataset from Google drive (or local disk).","metadata":{"id":"HhQN9DpQkUqT"}},{"cell_type":"code","source":"chinese_dataset = load_dataset('json', data_files={'train': JP_TRAIN_FILE, 'validation':JP_VALIDATION_FILE, 'test': JP_TEST_FILE})\nprint(chinese_dataset)\nprint(chinese_dataset[\"test\"][2][\"text\"])","metadata":{"id":"SiGEfoUMkUqT","outputId":"66d7b579-4295-41e9-9b75-60e2c941d378","execution":{"iopub.status.busy":"2024-09-13T16:02:22.140311Z","iopub.execute_input":"2024-09-13T16:02:22.141551Z","iopub.status.idle":"2024-09-13T16:02:22.518874Z","shell.execute_reply.started":"2024-09-13T16:02:22.141500Z","shell.execute_reply":"2024-09-13T16:02:22.517899Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b99edac32b947ae9b23ff73d1d16f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef598fda7154d668b0544d3ef406407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad300ed5fea4cb7a6477ef75a578bf4"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 428\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 9\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 9\n    })\n})\n昔々、ある日、深い森に住んでいた一匹のリスがいました。リスの名前はコタロウで、彼は森中を素早く駆け回って遊んでいました。ある日、森に嵐がやってきて、木々が激しく揺れ始めました。コタロウは急いで仲間たちを安全な場所へ導き、嵐が過ぎ去るまで皆を守りました。コタロウの勇気は動物たちに感謝され、彼の名は森中で語り継がれることになりました。\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We tokenize the raw text using Llama-2's tokenizer and group the tokenized text as inputs.","metadata":{"id":"uAcSghJgkUqT"}},{"cell_type":"code","source":"block_size = 380\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"])\n\ndef group_texts(examples):\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"id":"tUTCHIqEWJRL","execution":{"iopub.status.busy":"2024-09-13T16:02:25.459307Z","iopub.execute_input":"2024-09-13T16:02:25.460175Z","iopub.status.idle":"2024-09-13T16:02:25.467095Z","shell.execute_reply.started":"2024-09-13T16:02:25.460135Z","shell.execute_reply":"2024-09-13T16:02:25.465949Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tokenized_zh_datasets = chinese_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\nlm_datasets = tokenized_zh_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=380,\n    num_proc=4,\n)","metadata":{"id":"dXoXO29hWY-v","execution":{"iopub.status.busy":"2024-09-13T16:02:27.550832Z","iopub.execute_input":"2024-09-13T16:02:27.551584Z","iopub.status.idle":"2024-09-13T16:02:31.041556Z","shell.execute_reply.started":"2024-09-13T16:02:27.551543Z","shell.execute_reply":"2024-09-13T16:02:31.040595Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/428 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ffd55d18f44f999b00efe6545b52dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c78927acc044b3ae7d0e1c8c808d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b309b67c76b54fa5a923b7cc94b2f169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/428 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ef075046b44f018df1032f3c18f29d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a2b21c9bb54936bc29e71d551c6964"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57350bda4e664ae5af966863de74b733"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"0_m505-vnSBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 💡Tutorials: TrainingArguments().\n\n**Important Training Hyper-parameters**\n- learning_rate: The initial learning rate for optimizer.\n- num_train_epochs: Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).\n- *_strategy: The evaluation/saving strategy to adopt during training. Possible values are:\n    - `\"no\"`: No evaluation/saving is done during training.\n    - `\"steps\"`: Evaluation/saving is done (and logged) every `eval_steps`.\n    - `\"epoch\"`: Evaluation/saving is done at the end of each epoch.\n- per_device_train_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n- per_device_eval_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n- save_total_limit: If a value is passed, will limit the total amount of checkpoints.\n\n\n---\n\nIf you do not understand `AdamW` optimizer and learning scheduler, you may use default settings.\n\n**Optimizer Hyper-parameters**\n- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.\n- adam_beta1: The beta1 hyperparameter for the [`AdamW`] optimizer.\n- adam_beta2: The beta2 hyperparameter for the [`AdamW`] optimizer.\n\n**Learning schedule**\n- lr_scheduler: The scheduler type to use.\n- warmup_ratio: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n\n[Explore more parameters here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments)","metadata":{"id":"-DUTC5J7nSZt"}},{"cell_type":"markdown","source":"#### Task 3 Playground\n\n---\n\n📚 Please just run the following code to do continual pre-training. Please try your best to tune the hyperparameters or collect more data to improve model performance.\n\n---","metadata":{"id":"R9OJF3VNhrGK"}},{"cell_type":"code","source":"# =========Pre-training hyperparameters, please feel free to tune them~=========\n# =Important=\nlr = 3e-5\nepochs = 300\nsave_steps=200\nstrategy=\"steps\"\ntrain_bsz = 24 # reduce batch size if you encountered out-of-memory errors.\neval_bsz = 16\n\n# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n# =Optimizer=\noptimizer = \"adamw_torch\"\nweight_decay = 0.01\nadam_beta1 = 0.9\nadam_beta2 = 0.98\n# =Learning scheduler=\nlr_scheduler = \"linear\"\nwarmup_ratio = 0.01\n# =========End of pre-training hyperparameters=========\n\n\ntraining_args = TrainingArguments(\n    \"llama-42m-zh-fairytales\",\n    evaluation_strategy = strategy,\n    eval_steps=save_steps,\n    save_strategy = strategy,\n    save_steps=save_steps,\n    logging_strategy=\"steps\",\n    logging_steps = 10,\n    learning_rate=lr,\n    weight_decay=weight_decay,\n    seed=42,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=eval_bsz,\n    save_total_limit=1,\n    optim = optimizer,\n    lr_scheduler_type = lr_scheduler,\n    adam_beta1 = adam_beta1,\n    adam_beta2 = adam_beta2,\n    warmup_ratio = warmup_ratio,\n    num_train_epochs = epochs,\n    report_to=None\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"id":"Lf6bS7lnbPMM","execution":{"iopub.status.busy":"2024-09-13T04:54:00.792083Z","iopub.execute_input":"2024-09-13T04:54:00.792473Z","iopub.status.idle":"2024-09-13T04:54:00.836975Z","shell.execute_reply.started":"2024-09-13T04:54:00.792436Z","shell.execute_reply":"2024-09-13T04:54:00.836085Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"id":"KCIR43fZcAa9","outputId":"adecb9fd-1375-42d9-f7b2-60b91e4dc1de","execution":{"iopub.status.busy":"2024-09-13T04:54:03.393758Z","iopub.execute_input":"2024-09-13T04:54:03.394141Z","iopub.status.idle":"2024-09-13T05:23:45.886423Z","shell.execute_reply.started":"2024-09-13T04:54:03.394105Z","shell.execute_reply":"2024-09-13T05:23:45.885421Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240913_045428-lf4n35d2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jackariamei-university-of-macau/huggingface/runs/lf4n35d2' target=\"_blank\">llama-42m-zh-fairytales</a></strong> to <a href='https://wandb.ai/jackariamei-university-of-macau/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jackariamei-university-of-macau/huggingface' target=\"_blank\">https://wandb.ai/jackariamei-university-of-macau/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jackariamei-university-of-macau/huggingface/runs/lf4n35d2' target=\"_blank\">https://wandb.ai/jackariamei-university-of-macau/huggingface/runs/lf4n35d2</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1800/1800 28:57, Epoch 300/300]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>4.183800</td>\n      <td>4.068296</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.431500</td>\n      <td>2.220478</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.029500</td>\n      <td>1.038053</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.410400</td>\n      <td>0.821115</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.144500</td>\n      <td>0.831284</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.041700</td>\n      <td>0.873175</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.014000</td>\n      <td>0.942895</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.007800</td>\n      <td>0.981071</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.006100</td>\n      <td>0.998302</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1800, training_loss=1.290800393279642, metrics={'train_runtime': 1780.902, 'train_samples_per_second': 41.777, 'train_steps_per_second': 1.011, 'total_flos': 4292639539200000.0, 'train_loss': 1.290800393279642, 'epoch': 300.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Load pre-trained model and try to generate mini-story in another language.","metadata":{"id":"TjA9aBL4kUqV"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device type: {device}\")\n\nnew_model_path = \"/kaggle/working/llama-42m-zh-fairytales/checkpoint-1800\" # saved checkpoint path\nmodel = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"lE_QVLCOkUqV","outputId":"cf3ddb1a-5e89-473d-eeb8-7e32d87b7cb1","execution":{"iopub.status.busy":"2024-09-13T05:24:18.115308Z","iopub.execute_input":"2024-09-13T05:24:18.115733Z","iopub.status.idle":"2024-09-13T05:24:18.371827Z","shell.execute_reply.started":"2024-09-13T05:24:18.115695Z","shell.execute_reply":"2024-09-13T05:24:18.370736Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Device type: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport gc\ngc.collect() \n\nif device == \"cuda\":\n    for i in range(torch.cuda.device_count()):\n        with torch.cuda.device(i):\n            torch.cuda.empty_cache()\n            print(f\"Cleared cache for GPU {i}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T05:24:19.927659Z","iopub.execute_input":"2024-09-13T05:24:19.928085Z","iopub.status.idle":"2024-09-13T05:24:20.625093Z","shell.execute_reply.started":"2024-09-13T05:24:19.928047Z","shell.execute_reply":"2024-09-13T05:24:20.623536Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Cleared cache for GPU 0\nCleared cache for GPU 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluate the PPL on Chinese text (or another language) again.\n\nYou will notice that we actually achieve a much lower PPL after continual pre-training.","metadata":{"id":"hlX2XilNkUqV"}},{"cell_type":"code","source":"data_file = JP_TEST_FILE\ntest_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n\nresults = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 4)\ndataset_ppl = results['mean_perplexity']\nprint(f\"Test Perplexity: {dataset_ppl:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"0Ob_g328cGq5","outputId":"e7f2b619-f3c8-4dc2-bbc2-76fac657b2b6","execution":{"iopub.status.busy":"2024-09-13T05:24:21.880384Z","iopub.execute_input":"2024-09-13T05:24:21.881076Z","iopub.status.idle":"2024-09-13T05:24:22.397006Z","shell.execute_reply.started":"2024-09-13T05:24:21.881032Z","shell.execute_reply":"2024-09-13T05:24:22.395736Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a60134542df49edbd574f52de0bb3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a3795d17a64bda92300d8963529902"}},"metadata":{}},{"name":"stdout","text":"Test Perplexity: 3.11\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\nThe original English base model was pre-trained on 2 million data samples. Considering we are using only 10,000 training samples (0.5% of the original pre-training data), the model can generate a few fluent sentences but may still struggle with long-text generation or common sense of other languages. You can try using more data or training steps depending on your computational resources.\n\n---","metadata":{"id":"Xgi1EMDycZUp"}},{"cell_type":"code","source":"prompt = \"君の名は\"\n\n# Decoding hyperparameters\nmax_new_tokens = 300\ndo_sample = True\ntemperature = 0.3\n\ntokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput_ids = model.generate(\n    tokenized_input,\n    max_new_tokens=max_new_tokens,\n    eos_token_id=1,\n    do_sample=do_sample,\n    temperature=temperature,\n)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"id":"Rf9ALF4CEX82","outputId":"66d35223-1871-4d92-a008-cd50f257db4c","execution":{"iopub.status.busy":"2024-09-13T05:27:55.258225Z","iopub.execute_input":"2024-09-13T05:27:55.258970Z","iopub.status.idle":"2024-09-13T05:27:57.231341Z","shell.execute_reply.started":"2024-09-13T05:27:55.258926Z","shell.execute_reply":"2024-09-13T05:27:57.230151Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<s> 君の名は、ある日、遠い語り力と彼の名前はオリオンで、狐が住んでいました。彼の名前はシロで、森の森中の森の中を彼は旅をしていました。ある日、森に危機が訪れ、シロはその知恵を使って森を守ることを決意しました。彼は皆と協力し、無事に森を守りました。モモの知恵と勇気は、動物たちに感謝され、彼の名は森の守護者として語り継がれることになりました。<s>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"JJYZCXX3kUqZ"},"execution_count":null,"outputs":[]}]}